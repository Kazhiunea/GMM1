# -*- coding: utf-8 -*-
"""1stUzd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fDsYyln1sg57ZqoX_oIjQ7lrHqEb9F4M

Importing things I will need

I am using PyTorch for this model
"""

import torch.nn as nn
import torch.nn.functional as F

import torchvision
import torch
from torchvision import datasets
from torchvision.datasets import SVHN
import torchvision.transforms as transforms
from torchvision.transforms import ToTensor
from torch.utils.data import random_split
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import random

"""Mounting Google Drive for later use (saving/loading model)"""

from google.colab import drive
drive.mount('/content/gdrive')

"""I am working with SVHN data set:
10 classes of 32x32 images containing numbers 0-9 (thus 10 classes indicating which number there is)

Transform functions used for manipulating the data I get from the DataSet. More info near each of the function.

Transforms are used to make the data more "interesting", change things up a bit and make the model train on more different samples, so it doesn't only learn to be good at the SVHN model (be good only with the train pictures and suck at random ones)
"""

# Transforms will modify pictures while training, making sure the net does not overfit.
# Note that here I did not normalize the input (the data loader returns it in range [0-1]) to avoid losing color information.
transformTrain = transforms.Compose([
  transforms.RandomHorizontalFlip(), # 50/50 chance for the image to be horizontally flipped
  transforms.Pad(2), # add 2 pixels of padding in each direction, going from 32x32 to 36x36
  transforms.RandomCrop((32, 32)), # take a random area in the 36x36 image, goes back to 32x32 (but we now may have borders on edges)
  transforms.RandomRotation(10), # rotate image randomly up to 10 degrees
  transforms.ToTensor(), # final op to turn it into a tensor so we can work with it [3][32][32]
])

# we always want to retain original images, and we don't re-scale dimensions or scale in our training data
transformTest = transforms.Compose([ transforms.ToTensor(), ])

"""Now, I am getting the data from the data set
aka Downloading the data set as whole_data

I decided to use 12k images from the set as validation data, thus train_data size will be whole_data - val_size

I will be using 2 workers for faster data loading. That said, it seems that the data loading is bottlenecked at the Disk, since having 1 worker "imports" 30-38 images/s, yet 2 workers peek at 41. The increase is there, but is barely noticible

I will work with image batches with batch size of 128 images/batch

Setting up the data loaders

Some Debugging
"""

#Downloading the Data
whole_data = SVHN(root='data/', download=True, transform=ToTensor())

#val_size sets aside some data, that I will used for testing the model
val_size = 12000
#train_size is all the data left after setting aside the testing data: SVHN - val_size = train_size
train_size = len(whole_data) - val_size

#Creating 2 non-overlapping data sets, from train_data
train_data, val_data = random_split(whole_data, [train_size, val_size])

train_data.dataset.transform = transforms.Compose([
  transforms.RandomHorizontalFlip(), # 50/50 chance for the image to be horizontally flipped
  transforms.Pad(2), # add 2 pixels of padding in each direction, going from 32x32 to 36x36
  transforms.RandomCrop((32, 32)), # take a random area in the 36x36 image, goes back to 32x32 (but we now may have borders on edges)
  transforms.RandomRotation(10), # rotate image randomly up to 10 degrees
  transforms.ToTensor(), # final op to turn it into a tensor so we can work with it [3][32][32]
])


#len(train_data), len(val_data)

# number of cores to be used for loading data
num_workers = 2

# how many image we process in a single batch. This helps utilize paralellization, and makes learning smoother.
batch_size = 128

# creating data loaders. It's a nice abstraction that allows us to focus on other things.
#train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)
#test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)

train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)
test_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, num_workers=num_workers)

#Sanity check - printing the numbers of data I'll be using
N_train = len(train_data)
N_test = len(val_data)

print("train data: {}, test data: {}".format(N_train, N_test))
print("data sample shape: {}".format(train_data[0][0].shape))
print("data range [{}, {}]".format(torch.min(train_data[0][0]), torch.max(train_data[0][0])))

"""This is a test to see how fast workers import the data, yet as mentioned before the difference between 1 and 2 workers is barely noticible"""

for data in tqdm(train_loader):
  pass

"""Just checking what images I have in train_data, nothing special or useful here"""

img = train_data[0][0].numpy()
plt.imshow(img.transpose(1,2,0));

"""The Convolutions for the model:

Defining the convolutions in a constructor, this needs to be done since the Conv2d and others have weights in them.

Conv2d parameters are:

channelsIn - How many inputs I will be providing

channelsOut - How many outputs I am expecting

Kernel size of 3

And additional padding of 1

This can be imagined as a neuron layer, where in the first layer I will have channelsIn neurons and the 2nd layer will have channelsOut neurons

forward() basically calls the defined Convlution layers.

MyNet is the fun part of the Convolution "making":

Again, defining the convolution logic as a constructor as well.

So the convolutions:

Firstly I "give" 3 images (this is because the image is turned into 3 RGB images) Images are basically halved in size. I put in 32image, I get 16image. 16image -> 8image, 8image -> 4image, 4image -> 2image.

Now I make every image into a neuron
"""

class ConvReluMaxpool(nn.Module):
    def __init__(self, channelsIn, channelsOut):
        super(ConvReluMaxpool, self).__init__()
        # first we have a 3x3 convolution. Since the input is bigger by 1 on each side than the output, we set padding to 1 to make the output same size as input.
        # 3x3 convolutions are well optimized and don't take much space, but have limited perceptive field (i.e. they don't see a lot of data - 3x3 square precisely)
        self.conv = nn.Conv2d(channelsIn, channelsOut, 3, padding = 1)
        # ReLU is a simple, fast non-linearity. It turns negative values to 0 and preserves positive values.
        # This means sparse neuron activation behavious can be introduced, making the network capable of making more "categorical" decisions
        # Non-linearities are also used to make the model non-linear, ar y = x*w+b is actually quite limited and linear even if you stack a lot of them.
        self.relu = nn.ReLU()
        # Finally, we use MaxPool to reduce the dimensions of the image width an height. stride is by default 2, so we take 2x2 area, return max value, move by 2 and repeat.
        # This also allows to introduce more categorical decision making, as less relevant data is discarded.
        self.maxpool = nn.MaxPool2d(2, 2)

    # here we simply use the initialized layers
    # Keep in mind that if you want seperate weights, you need a separately initialized layer
    def forward(self, x):
        x = self.conv(x)
        x = self.relu(x)
        x = self.maxpool(x)
        return x

# Simple network
# Networks can be interpreted as more complex layers - the layer we created above also inherits from nn.Module.
class MyNet(nn.Module):
    # we use a couple of parameters:
    # channel_mult is a multiplier for the channel amount for each layer - higher value means more channels, larger and slower network, but more capacity to learn. Layers with more channels are also called "wider"
    # fc_size determines the size of the fully connected layer at the end. This layers collects the convolution outputs and finds which of them map to which labels. The amount of data processed is quickly reduced here.
    def __init__(self, channel_mult, fc_size):
        super(MyNet, self).__init__()

        # Putting all layers in Sequantial can make the code clean.
        # However, it can make accessing the layers more difficult later.
        self.layers = nn.Sequential(
            # the input and output shapes for the layer are commented below. They are NCHW - Number of elements (batch size), number of Channels, Height and Width
            ConvReluMaxpool(3, 1 * channel_mult),                 # 64, 3, 32, 32 -> 64, 1 * channel_mult, 16, 16. As you should remember, ConvReluMaxpool has MaxPool which halves width and height.
            ConvReluMaxpool(1 * channel_mult, 2 * channel_mult),  # 64, 1 * channel_mult, 16, 16 -> 64, 2 * channel_mult, 8, 8
            ConvReluMaxpool(2 * channel_mult, 4 * channel_mult),  # 64, 2 * channel_mult, 8, 8 -> 64, 4 * channel_mult, 4, 4
            ConvReluMaxpool(4 * channel_mult, 8 * channel_mult),  # 64, 4 * channel_mult, 4, 4 -> 64, 8 * channel_mult, 2, 2

            # At this point the image is 2x2, so using 3x3 convolution doesn't make much sense and is not efficient.
            # We now flatten the output (make it [N,C*H*W]) and use a fully connected layer to select which features get mapped
            nn.Flatten(start_dim = 1, end_dim = -1), #[128][64][2][2] -> [128][4*64] -> [128][256]
            nn.Linear(8 * channel_mult * 2 * 2, fc_size),
            nn.ReLU(), # Non-linearities are applicable to all weight-based layers
            nn.Linear(fc_size, 10), # 10 = number of classes. Non non-linearity the the end. Could use sigmoid to maske it in range [0:1]
        )

    # when running the network we just call the sequential set of layers we created.
    def forward(self, x):
        return self.layers(x)

"""Calling the convolution"""

# we create a layer with channel multiplier of 8 (so the conv channels will range from 8 to 64, which is very small, but could be suitable for cifar32 which is really a toy example of a dataset)
net_fp32 = MyNet(8, 128)
net_fp32 # just typing a variable prints it - here we print our network as another sanity check

"""This function gets the number of images in a batch, when the images were "guessed" correctly. Ex: model gets 128 images, gets 50 correct, the image returns 50"""

# this calculates classification accuracy using the class probabilities and true labels
# you might prefer using F1 metric, we are keeping it simple to a maximum here
def get_acc(outputs, labels):
    with torch.no_grad():
        outputLabels = torch.argmax(outputs, 1) # get the highest prediction for a batch element [0.1][0.2][0.3][0.4]
        return torch.sum((labels == outputLabels).float()) # accuracy is the mean of matching labels ()

"""Setting the decive to GPU"""

# Utility function - puts all elements of a set to specificed device.
def set_to_device(data, device):
    return (d.to(device) for d in data)

"""Training function

Using the ADAM optimizer for the backpropogation
"""

DEVICE = 'cuda' # try using 'cuda' for big speed-up (don't forget Runtime -> Change Runtime Type -> GPU)

# we define a training function
# this takes:
# net: the network we will train
# n_epochs - how long do we train. 1 epoch is 1 time going through the entire dataset
# lr: learning rate
def train(net, n_epochs, lr):
    print("Training with LR={}".format(lr))
    net.to(DEVICE) # on cpu by default, but if you changed DEVICE to CUDE this will load accordingly

    # we are now familiar with MSE, so we use this, but do look into other ones too when doing projects
    loss_func = nn.MSELoss()
    # Adam is an optimizer - it determines how the weights are adjusted based on loss.
    # optimizers include things like momentum to make learning more smooth and avoid getting stuck in local minimums
    optimizer = torch.optim.Adam(net.parameters(), lr=lr) # we pass out parameters and learning rate, keep others at default.

    for epoch in range(0, n_epochs):
        # monitor losses and accuracies for summaries later
        train_loss = 0.0
        train_acc = 0.0
        test_loss = 0.0
        test_acc = 0.0

        ###############################
        # train the model for 1 epoch #
        ###############################
        for data in train_loader: #takes 128images x3               [128, 3, 32, 32][128, 1]
            # data point contains the image and true label - we transfer them to our device and split into two variables
            # each data point is a batch of 64 images and outputs
            images, labels = set_to_device(data, DEVICE) #[128][3][32][32] AND [128]
            # because gradient is accumulated in pytroch, we have to reset it at a beginning of a batch
            optimizer.zero_grad() #grad is set to 0, to not overlap the calculated grad
            # calculate predicted outputs
            outputs = net(images)  #net_fp32 == net, images (tensor) get sent to forward(self, x) as x. Convolutions of the images begins. I get [128][10] (batch + values of final 10 neurons)
            # loss function is squared difference between out prediction and true labels
            # keep in mind training aims to minimize the loss, not maximize
            # the one-hot function turns integer label into a vector of 0's, with the value at index of the integer is 1. This makes it identical to our network output format.
            loss = loss_func(outputs, F.one_hot(labels, 10).float())
            #loss = loss_func([128][10], F.one_hot([128], 10).float())
            #[128] = 0
            #[10] = 0
            # [0][0] = 0.0004578421 -> one_hot 
            

            # save changes to info variables to be printed later
            train_loss += loss.item() * images.size(0)
            train_acc += get_acc(outputs, labels)

            # now we the pytoch to calculate the gradients and for the optimizer to change weights based on those gradients
            loss.backward()
            optimizer.step()
        
        # we calculate accuracy here
        for data in test_loader:
            # this line means we don't need to calculate gradient here. Avoid test data leakage into training, and speeds inference up.
            # no comments and higher compactness as we covered these things just now
            with torch.no_grad():
              images, labels = set_to_device(data, DEVICE)
              outputs = net(images)
              loss = loss_func(outputs, F.one_hot(labels, 10).float())
              test_loss += loss.item() * images.size(0)
              test_acc += get_acc(outputs, labels)

        print('Epoch: {} | Train Loss: {:.6f} | Train Acc: {:.3f} | Test Loss: {:.6f} | Test Acc: {:.3f}'.format(
            epoch, 
            train_loss / N_train,
            train_acc / N_train,
            test_loss / N_test,
            test_acc / N_test,
            ))

# Learning Rate Decay is usually passed to training parameters, and it reduces LR every epoch.
# Here we simply train multiple times with different LRs, which also helps us illustrate some points.

# high pace initial training.
# You should notice training slow down significantly by epoch 30 due to the large step size overstepping the minimum.
# Large initial LR in coombination with momentum (which Adam incorporates) also helps to step over local minimums. 
train(net_fp32, 30, 1e-3)

# smaller LR allows for better precision, but would be slower if used from the start
# notice how training speeds up again (and slows down again as it goes) as it starts training with 1e-4 LR
#train(net_fp32, 20, 1e-4)

# small LR allows for final calibration. Beware of overfitting! We have data augmentation to deal with that, but that does not measn we are immune.
# note that deep networks need smaller LR in general. 1e-5 in a ResNet could be a starting LR.
#train(net_fp32, 10, 1e-5)

"""Saving the model to drive"""

torch.save(net_fp32.state_dict(),'gdrive/MyDrive/gmm/cnn-modelis.pth')

"""Getting the model to drive"""

net_fp32.load_state_dict(torch.load('gdrive/MyDrive/gmm/cnn-modelis.pth'))

"""I don't really think this is used anymore"""

def test_mock(test_loader):
  for data in test_loader:
            # this line means we don't need to calculate gradient here. Avoid test data leakage into training, and speeds inference up.
            # no comments and higher compactness as we covered these things just now
            with torch.no_grad():
              net_fp32.eval()
              images, labels = set_to_device(data, DEVICE)
              outputs = net_fp32(images)
              print(outputs)
              net_fp32.train()
              # loss = loss_func(outputs, F.one_hot(labels, 10).float())
              # test_loss += loss.item() * images.size(0)
              # test_acc += get_acc(outputs, labels)

"""Getting a random image from the whole_data, seeing what the image is, what the number should be, what number the model guesses and how "sure" it is"""

image, label = whole_data[random.randint(0,len(whole_data))]
#torch.ToTensor(image)

plt.imshow(image.numpy().transpose(1,2,0));
print(label)

labels = net_fp32(image[None])

print(sorted(enumerate(list(labels.detach().numpy()[0])),key=lambda x:-x[1])[0])
#print(sorted(list(enumerate(list(labels.detach().numpy()[0]))),key=lambda x:-x[1])[0])

